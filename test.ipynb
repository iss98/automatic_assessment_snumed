{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "folder_path = \"data/response\"\n",
    "csv_files = [file for file in os.listdir(folder_path) if file.endswith(\".csv\")]\n",
    "\n",
    "column_index = 1\n",
    "output_folder = \"data/text_data\"\n",
    "\n",
    "for file_name in csv_files:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    output_file_name = os.path.splitext(file_name)[0]+\".txt\"\n",
    "    output_file_path = os.path.join(output_folder, output_file_name)\n",
    "\n",
    "    with open(file_path, \"r\") as csv_file, open(output_file_path, \"w\") as txt_file:\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        next(csv_reader)\n",
    "\n",
    "        for row in csv_reader :\n",
    "            if row != \"\":\n",
    "                column_value = row[column_index]\n",
    "                txt_file.write(column_value + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "data_dir = \"data/text_data\"\n",
    "paths = [str(x) for x in Path(data_dir).glob(\"1-1.txt\")]\n",
    "corpus = \",\".join(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=data/text_data/1-1.txt --model_prefix=sample --model_type=bpe --max_sentence_length=999999 --pad_id=0 --pad_piece=<pad> --unk_id=1 --unk_piece=<unk> --bos_id=2 --bos_piece=<s> --eos_id=3 --eos_piece=</s> --user_defined_symbols=<sep>,<cls>,<mask>\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: data/text_data/1-1.txt\n",
      "  input_format: \n",
      "  model_prefix: sample\n",
      "  model_type: BPE\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 999999\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: <sep>\n",
      "  user_defined_symbols: <cls>\n",
      "  user_defined_symbols: <mask>\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 1\n",
      "  bos_id: 2\n",
      "  eos_id: 3\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: data/text_data/1-1.txt\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 233 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <sep>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <cls>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <mask>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=6781\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9558% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=53\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999558\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 233 sentences.\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 233\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 49\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=732 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=20 all=126 active=72 piece=▁)\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=40 all=125 active=71 piece=▁지수법칙\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=60 all=119 active=65 piece=}/{\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=80 all=100 active=46 piece=\\\\\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=100 all=80 active=26 piece=이다\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=0 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=120 all=60 active=6 piece=^{19\n",
      "bpe_model_trainer.cc(252) LOG(WARNING) No valid symbol found\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: sample.model\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Internal: /Users/runner/work/sentencepiece/sentencepiece/src/trainer_interface.cc(661) [(trainer_spec_.vocab_size()) == (model_proto->pieces_size())] Vocabulary size too high (8000). Please set it to a value <= 185.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m prefix \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msample\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m vocab_size \u001b[39m=\u001b[39m \u001b[39m60\u001b[39m\n\u001b[0;32m----> 3\u001b[0m spm\u001b[39m.\u001b[39;49mSentencePieceTrainer\u001b[39m.\u001b[39;49mtrain(\n\u001b[1;32m      4\u001b[0m     \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m--input=\u001b[39;49m\u001b[39m{\u001b[39;49;00mcorpus\u001b[39m}\u001b[39;49;00m\u001b[39m --model_prefix=\u001b[39;49m\u001b[39m{\u001b[39;49;00mprefix\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m \n\u001b[1;32m      5\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m --model_type=bpe\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m\n\u001b[1;32m      6\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m --max_sentence_length=999999\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39m# 문장 최대 길이 (너무 길면 에러발생)\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m --pad_id=0 --pad_piece=<pad>\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39m# pad (0)\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m --unk_id=1 --unk_piece=<unk>\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39m# unknown (1)\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m --bos_id=2 --bos_piece=<s>\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39m# begin of sequence (2)\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m --eos_id=3 --eos_piece=</s>\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39m# end of sequence (3)\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m --user_defined_symbols=<sep>,<cls>,<mask>\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/assessment/lib/python3.9/site-packages/sentencepiece/__init__.py:989\u001b[0m, in \u001b[0;36mSentencePieceTrainer.Train\u001b[0;34m(arg, logstream, **kwargs)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    987\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mTrain\u001b[39m(arg\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, logstream\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    988\u001b[0m   \u001b[39mwith\u001b[39;00m _LogStream(ostream\u001b[39m=\u001b[39mlogstream):\n\u001b[0;32m--> 989\u001b[0m     SentencePieceTrainer\u001b[39m.\u001b[39;49m_Train(arg\u001b[39m=\u001b[39;49marg, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/assessment/lib/python3.9/site-packages/sentencepiece/__init__.py:945\u001b[0m, in \u001b[0;36mSentencePieceTrainer._Train\u001b[0;34m(arg, **kwargs)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Train Sentencepiece model. Accept both kwargs and legacy string arg.\"\"\"\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[39mif\u001b[39;00m arg \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mtype\u001b[39m(arg) \u001b[39mis\u001b[39;00m \u001b[39mstr\u001b[39m:\n\u001b[0;32m--> 945\u001b[0m   \u001b[39mreturn\u001b[39;00m SentencePieceTrainer\u001b[39m.\u001b[39;49m_TrainFromString(arg)\n\u001b[1;32m    947\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_encode\u001b[39m(value):\n\u001b[1;32m    948\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Encode value to CSV..\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/assessment/lib/python3.9/site-packages/sentencepiece/__init__.py:923\u001b[0m, in \u001b[0;36mSentencePieceTrainer._TrainFromString\u001b[0;34m(arg)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    922\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_TrainFromString\u001b[39m(arg):\n\u001b[0;32m--> 923\u001b[0m     \u001b[39mreturn\u001b[39;00m _sentencepiece\u001b[39m.\u001b[39;49mSentencePieceTrainer__TrainFromString(arg)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Internal: /Users/runner/work/sentencepiece/sentencepiece/src/trainer_interface.cc(661) [(trainer_spec_.vocab_size()) == (model_proto->pieces_size())] Vocabulary size too high (8000). Please set it to a value <= 185."
     ]
    }
   ],
   "source": [
    "prefix = \"sample\"\n",
    "vocab_size = 60\n",
    "spm.SentencePieceTrainer.train(\n",
    "    f\"--input={corpus} --model_prefix={prefix} --vocab_size={vocab_size + 7}\" + \n",
    "    \" --model_type=bpe\" +\n",
    "    \" --max_sentence_length=999999\" + # 문장 최대 길이 (너무 길면 에러발생)\n",
    "    \" --pad_id=0 --pad_piece=<pad>\" + # pad (0)\n",
    "    \" --unk_id=1 --unk_piece=<unk>\" + # unknown (1)\n",
    "    \" --bos_id=2 --bos_piece=<s>\" + # begin of sequence (2)\n",
    "    \" --eos_id=3 --eos_piece=</s>\" + # end of sequence (3)\n",
    "    \" --user_defined_symbols=<sep>,<cls>,<mask>\") # 사용자 정의 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'token/save/sample.model'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.move(\"sample.model\", \"token/save/sample.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'token/save/sample.vocab'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.move(\"sample.vocab\", \"token/save/sample.vocab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('save/token/sample/tokenizer_config.json',\n",
       " 'save/token/sample/special_tokens_map.json',\n",
       " 'save/token/sample/spiece.model',\n",
       " 'save/token/sample/added_tokens.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "tokenizer = T5Tokenizer(vocab_file=\"token/save/sample.model\")\n",
    "tokenizer.save_pretrained(\"save/token/sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp = BertWordPieceTokenizer(lowercase = False, strip_accents = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_file = \"data/text_data/1-1.txt\"\n",
    "\n",
    "wp.train(files = data_file,\n",
    "         vocab_size = vocab_size,\n",
    "         limit_alphabet = 1000,\n",
    "         min_frequency = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['token/save/sample_bw-vocab.txt']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wp.save_model(\"token/save\", \"sample_bw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'join'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m A \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m A\u001b[39m.\u001b[39;49mjoin()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'join'"
     ]
    }
   ],
   "source": [
    "A = ['a', 'b']\n",
    "A.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "rnn = nn.RNN(10,10, batch_first = True)\n",
    "x = torch.randn(2, 3,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "output,_ = rnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 10])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4072,  0.8018,  0.6692, -0.2829,  0.2577,  0.5499, -0.5442, -0.4578,\n",
       "         -0.1705, -0.6596],\n",
       "        [ 0.2618,  0.5467,  0.5199,  0.2080,  0.6802,  0.3067, -0.4009, -0.4412,\n",
       "          0.0210,  0.0589],\n",
       "        [-0.4238,  0.4948, -0.8356,  0.1908,  0.7024, -0.1275,  0.5584,  0.6959,\n",
       "          0.0582, -0.2872]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/vvr001snu/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=data/text_data/1-1.txt  --model_prefix=1-1_rnn_sp_60 --vocab_size=60 --model_type=bpe --max_sentence_length=999999 --pad_id=0 --pad_piece=<pad> --unk_id=1 --unk_piece=<unk> --bos_id=2 --bos_piece=<s> --eos_id=3 --eos_piece=</s> --user_defined_symbols=<sep>,<cls>,<mask>\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: data/text_data/1-1.txt\n",
      "  input_format: \n",
      "  model_prefix: 1-1_rnn_sp_60\n",
      "  model_type: BPE\n",
      "  vocab_size: 60\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 999999\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: <sep>\n",
      "  user_defined_symbols: <cls>\n",
      "  user_defined_symbols: <mask>\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 1\n",
      "  bos_id: 2\n",
      "  eos_id: 3\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: data/text_data/1-1.txt\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 233 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <sep>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <cls>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <mask>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=6781\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9558% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=53\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999558\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 233 sentences.\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 233\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 49\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: 1-1_rnn_sp_60.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: 1-1_rnn_sp_60.vocab\n",
      "233it [00:00, 11175.73it/s]\n",
      "================================================\n",
      "데이터 로드 성공\n",
      "데이터의 크기(학생들 풀이의 개수) : 233\n",
      "모델 정보 : 1-1_rnn_sp_60\n",
      "================================================\n",
      "================================================\n",
      "모델설정 완료 학습 시작\n",
      "파일 저장 위치 : save/1-1_rnn_sp_60.pt\n",
      "================================================\n",
      "100%|███████████████████████████████████████████| 12/12 [00:01<00:00,  8.20it/s]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:00<00:00, 14.72it/s]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:00<00:00, 14.55it/s]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:00<00:00, 14.65it/s]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:00<00:00, 14.80it/s]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:00<00:00, 14.54it/s]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:00<00:00, 14.30it/s]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:00<00:00, 14.91it/s]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:00<00:00, 14.55it/s]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:00<00:00, 14.44it/s]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:00<00:00, 14.77it/s]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:00<00:00, 14.62it/s]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:00<00:00, 14.62it/s]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:00<00:00, 14.75it/s]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:00<00:00, 14.69it/s]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:00<00:00, 14.47it/s]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:00<00:00, 14.58it/s]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:00<00:00, 14.63it/s]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:00<00:00, 14.69it/s]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:00<00:00, 14.71it/s]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:00<00:00, 14.28it/s]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:00<00:00, 14.27it/s]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:00<00:00, 14.65it/s]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:00<00:00, 14.32it/s]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:00<00:00, 14.31it/s]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:00<00:00, 13.83it/s]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:00<00:00, 14.28it/s]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:00<00:00, 14.50it/s]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:00<00:00, 14.52it/s]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:00<00:00, 14.56it/s]\n",
      "100%|███████████████████████████████████████████| 12/12 [00:00<00:00, 14.45it/s]\n",
      " 33%|██████████████▋                             | 4/12 [00:00<00:00, 14.18it/s]^C\n",
      " 58%|█████████████████████████▋                  | 7/12 [00:00<00:00, 12.81it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/vvr001snu/Desktop/Automatic Assessment/train.py\", line 87, in <module>\n",
      "    loss.backward()\n",
      "  File \"/opt/miniconda3/envs/assessment/lib/python3.9/site-packages/torch/_tensor.py\", line 487, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/opt/miniconda3/envs/assessment/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 200, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py --item 1-1 --model rnn --tokenizer sp --device mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
